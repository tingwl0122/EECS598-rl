{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j9CpC88gJ5fU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db52b41c-12cb-4924-df2c-4b48e6da98b2","executionInfo":{"status":"ok","timestamp":1647842393056,"user_tz":240,"elapsed":12183,"user":{"displayName":"威","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05844397493681881106"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Box2D\n","  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 122 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 153 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 163 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 194 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 204 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 215 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 235 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 245 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 256 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 266 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 276 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 286 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 296 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 307 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 317 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 327 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 337 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 348 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 358 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 378 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 389 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 399 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 419 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 430 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 440 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 460 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 471 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 481 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 501 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 512 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 522 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 532 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 542 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 552 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 563 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 573 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 583 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 593 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 604 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 614 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 624 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 634 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 645 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 655 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 665 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 675 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 686 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 696 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 706 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 716 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 727 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 737 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 747 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 757 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 768 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 778 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 788 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 798 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 808 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 819 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 829 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 839 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 849 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 860 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 870 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 880 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 890 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 901 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 911 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 921 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 931 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 942 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 952 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 962 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 972 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 983 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 993 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3 MB 8.1 MB/s \n","\u001b[?25hInstalling collected packages: Box2D\n","Successfully installed Box2D-2.3.10\n"]}],"source":["!pip install Box2D\n","import numpy as np\n","import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import deque\n","import random\n","import torch.optim as optim"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-FB93J-jcjd","executionInfo":{"status":"ok","timestamp":1647842410157,"user_tz":240,"elapsed":14276,"user":{"displayName":"威","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05844397493681881106"}},"outputId":"1c51be3a-4642-4c14-96da-8ba169ec8408"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"nbgrader":{"grade":false,"grade_id":"question_code","locked":false,"schema_version":1,"solution":true},"id":"N9ds4SfvJ5fW"},"outputs":[],"source":["# Actor Neural Network\n","class Actor(nn.Module):\n","    def __init__(self, state_size, action_size, seed, fc_units=512, fc1_units=512):\n","        super(Actor, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.fc1 = nn.Linear(state_size, fc_units)\n","        self.fc2 = nn.Linear(fc_units, fc1_units)\n","        self.fc3 = nn.Linear(fc1_units, action_size)\n","\n","    def forward(self, state):\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        return F.torch.tanh(self.fc3(x))\n","\n","# Q1-Q2-Critic Neural Network\n","\n","class Critic(nn.Module):\n","    \"\"\"\n","    Args:\n","        state_size: state dimension\n","        action_size: action dimension\n","        fc_units: number of neurons in one fully connected hidden layer\n","    \"\"\"\n","    def __init__(self, state_size, action_size, seed, fc1_units=512, fc2_units=512):\n","        super(Critic, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(state_size + action_size, fc1_units)\n","        self.l2 = nn.Linear(fc1_units, fc2_units)\n","        self.l3 = nn.Linear(fc2_units, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(state_size + action_size, fc1_units)\n","        self.l5 = nn.Linear(fc1_units, fc2_units)\n","        self.l6 = nn.Linear(fc2_units, 1)\n","\n","    def forward(self, state, action):\n","        \"\"\"\n","        Args:\n","            state: torch.Tensor with shape (batch_size, state_size)\n","            action: torch.Tensor with shape (batch_size, action_size)\n","        Returns:\n","            x_1: torch.Tensor with shape (batch_size, 1)\n","            x_2: torch.Tensor with shape (batch_size, 1)\n","        \"\"\"\n","        xa = torch.cat([state, action], 1)\n","\n","        x1 = F.relu(self.l1(xa))\n","        x1 = F.relu(self.l2(x1))\n","        x1 = self.l3(x1)\n","\n","        x2 = F.relu(self.l4(xa))\n","        x2 = F.relu(self.l5(x2))\n","        x2 = self.l6(x2)\n","\n","        return x1, x2\n","\n","\n","class SysModel(nn.Module):\n","    def __init__(self, state_size, action_size, fc1_units=400, fc2_units=300):\n","        super(SysModel, self).__init__()\n","        self.l1 = nn.Linear(state_size + action_size, fc1_units)\n","        self.l2 = nn.Linear(fc1_units, fc2_units)\n","        self.l3 = nn.Linear(fc2_units, state_size)\n","\n","\n","    def forward(self, state, action):\n","        \"\"\"\n","        Args:\n","            state: torch.Tensor with shape (batch_size, state_size)\n","            action: torch.Tensor with shape (batch_size, action_size)\n","        Returns:\n","            state: torch.Tensor with shape (batch_size, state_size)\n","        \"\"\"\n","        xa = torch.cat([state, action], 1)\n","        x1 = F.relu(self.l1(xa))\n","        x1 = F.relu(self.l2(x1))\n","        x1 = self.l3(x1)\n","\n","        return x1\n","\n","\n","class RModel(nn.Module):\n","    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=256):\n","        super(RModel, self).__init__()\n","        # input s_t, a_t, s_{t+1}\n","        self.l1 = nn.Linear(2 * state_size + action_size, fc1_units)\n","        self.l2 = nn.Linear(fc1_units,fc2_units)\n","        self.l3 = nn.Linear(fc2_units, 1)\n","\n","    def forward(self, state, next_state, action):\n","      ssa = torch.cat([state, next_state, action], 1)\n","      r1 = F.relu(self.l1(ssa))\n","      r1 = F.relu(self.l2(r1))\n","      r1 = self.l3(r1)\n","      return r1\n","\n","\"\"\"\n","TD3_FORK AGENT\n","\"\"\"\n","\n","class Agent:\n","    def __init__(\n","        self,\n","        load = False,\n","        gamma = 0.99, #discount factor\n","        lr_actor = 3e-4,  # learning rate for actor network\n","        lr_critic = 3e-4, # learning rate for critic network\n","        lr_sysmodel = 3e-4, # learning rate for system network\n","\n","        lr_rmodel = 3e-4,\n","\n","        batch_size = 100,  # mini-batch size\n","        buffer_capacity = 1000000, # reply buffer capacitty\n","        tau = 0.005,  #target network update factor\n","        random_seed = np.random.randint(1,10000),  #random seed\n","        policy_noise=0.2,   # noise added to actor\n","        std_noise = 0.1,    # standard deviation for smoothing noise added to target policy\n","        noise_clip=0.5,   #noise bound\n","        policy_freq=2, #target network update period\n","        threshold = 0.01 #threshold of monitoring weights\n","    ):\n","        self.device = torch.device(\"cuda\")\n","        self.env = gym.make('BipedalWalkerHardcore-v3')\n","        self.create_actor()\n","        self.create_critic()\n","        self.create_sysmodel()\n","\n","        self.create_rmodel()\n","\n","        self.act_opt = optim.Adam(self.actor.parameters(), lr=lr_actor)\n","        self.crt_opt = optim.Adam(self.critic.parameters(), lr=lr_critic)\n","        self.sys_opt = optim.Adam(self.sysmodel.parameters(), lr=lr_sysmodel) #define system model\n","\n","        self.r_opt = optim.Adam(self.rmodel.parameters(), lr=lr_rmodel) #define system reward model\n","\n","        self.set_weights()\n","        self.replay_memory_buffer = deque(maxlen = buffer_capacity)\n","        self.replay_memory_bufferd_dis = deque(maxlen = buffer_capacity)\n","        self.batch_size = batch_size\n","        self.tau = tau\n","        self.policy_freq = policy_freq\n","        self.gamma = gamma\n","        self.upper_bound = self.env.action_space.high[0] #action space upper bound\n","        self.lower_bound = self.env.action_space.low[0]  #action space lower bound\n","        self.obs_upper_bound = self.env.observation_space.high[0] #state space upper bound\n","        self.obs_lower_bound = self.env.observation_space.low[0]  #state space lower bound\n","        self.policy_noise = policy_noise\n","        self.noise_clip = noise_clip\n","        self.std_noise = std_noise\n","\n","        self.threshold = threshold\n","\n","\n","    def create_actor(self):\n","        params = {\n","            'state_size':      self.env.observation_space.shape[0],\n","            'action_size':     self.env.action_space.shape[0],\n","            'seed':            88\n","        }\n","        self.actor = Actor(**params).to(self.device)\n","        self.actor_target = Actor(**params).to(self.device)\n","\n","    def create_critic(self):\n","        params = {\n","            'state_size':      self.env.observation_space.shape[0],\n","            'action_size':     self.env.action_space.shape[0],\n","            'seed':            88\n","        }\n","        self.critic = Critic(**params).to(self.device)\n","        self.critic_target = Critic(**params).to(self.device)\n","\n","    def create_sysmodel(self):\n","        params = {\n","            'state_size':      self.env.observation_space.shape[0],\n","            'action_size':     self.env.action_space.shape[0]\n","        }\n","        self.sysmodel = SysModel(**params).to(self.device)\n","        #self.sysmodel.apply(self.init_weights)\n","\n","    def create_rmodel(self):\n","        params = {\n","            'state_size':      self.env.observation_space.shape[0],\n","            'action_size':     self.env.action_space.shape[0]\n","        }\n","        self.rmodel = RModel(**params).to(self.device)\n","\n","    def set_weights(self):\n","        self.actor_target.load_state_dict(self.actor.state_dict())\n","        self.critic_target.load_state_dict(self.critic.state_dict())\n","\n","\n","    def add_to_replay_memory(self, transition, buffername):\n","        #add samples to replay memory\n","        buffername.append(transition)\n","\n","    def get_random_sample_from_replay_mem(self, buffername):\n","        #random samples from replay memory\n","        random_sample = random.sample(buffername, self.batch_size)\n","        return random_sample\n","\n","    def init_weights(self,model):\n","        if type(model) == nn.Linear:\n","          torch.nn.init.xavier_uniform_(model.weight)\n","          model.bias.data.fill_(0.001)\n","\n","\n","    def learn_and_update_weights_by_replay(self,training_iterations, weight, totrain):\n","        if len(self.replay_memory_buffer) < 1e4:\n","            return 1\n","        for it in range(training_iterations):\n","            mini_batch = self.get_random_sample_from_replay_mem(self.replay_memory_buffer)\n","            state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float().to(self.device)\n","            action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).float().to(self.device)\n","            reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float().to(self.device)\n","            next_state_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float().to(self.device)\n","            done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float().to(self.device)\n","\n","            # Train Critic\n","            with torch.no_grad():\n","              target_actions = self.actor_target(next_state_batch)\n","              offset_noises = torch.FloatTensor(action_batch.shape).data.normal_(0, self.policy_noise).to(self.device)\n","\n","              # Clip noise\n","              offset_noises = offset_noises.clamp(-self.noise_clip, self.noise_clip)\n","              target_actions = (target_actions + offset_noises).clamp(self.lower_bound, self.upper_bound)\n","\n","              # Compute the target Q value\n","              Q_targets1, Q_targets2 = self.critic_target(next_state_batch, target_actions)\n","              Q_targets = torch.min(Q_targets1, Q_targets2)\n","              Q_targets = reward_batch + self.gamma * Q_targets * (1 - done_list)\n","\n","            # Compute current Q estimates\n","            current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","            # Compute critic loss\n","            critic_loss = F.mse_loss(current_Q1, Q_targets.detach()) + F.mse_loss(current_Q2, Q_targets.detach())\n","            # Optimize the critic\n","            self.crt_opt.zero_grad()\n","            critic_loss.backward()\n","            self.crt_opt.step()\n","\n","\n","            \"\"\"Train Sysmodel\"\"\"\n","            loss_fn = torch.nn.SmoothL1Loss()\n","            next_state_pred = self.sysmodel(state_batch, action_batch).clamp(self.obs_lower_bound, self.obs_upper_bound)\n","            sys_loss = loss_fn(next_state_pred, next_state_batch.detach())\n","\n","            self.sys_opt.zero_grad()\n","            sys_loss.backward()\n","            self.sys_opt.step()\n","            sys_loss_num = sys_loss.item()\n","\n","\n","            \"\"\"Train Reward model\"\"\"\n","\n","\n","            loss_fn_1 = torch.nn.MSELoss()\n","            pred_r = self.rmodel(state_batch, next_state_batch, action_batch)\n","            r_loss = loss_fn_1(pred_r, reward_batch.detach())\n","\n","            self.r_opt.zero_grad()\n","            r_loss.backward()\n","            self.r_opt.step()\n","            r_loss_num = r_loss.item()\n","\n","\n","            usesys = 1 if sys_loss_num < self.threshold  else 0\n","\n","\n","\n","            \"\"\"Train Actor\"\"\"\n","            # Train Actor\n","            # Delayed policy updates\n","            # Update self.actor once every policy_delay times for each update of self.critic\n","\n","            if it % self.policy_freq == 0:\n","\n","              first_loss = torch.mean(self.critic(state_batch, self.actor(state_batch))[0])\n","\n","              if usesys == 1:\n","                # telta s_{t+1}\n","                pred_next_state = self.sysmodel(state_batch, self.actor(state_batch)).clamp(self.obs_lower_bound,self.obs_upper_bound)\n","                # second term\n","                # r ( s_{t}, s_{t+1}, actor(s_{t}))\n","                pred_next_r = self.rmodel(state_batch.detach() ,pred_next_state.detach(), self.actor(state_batch.detach()))\n","\n","                # telta s_{t+2}\n","                pred_next_next_state = self.sysmodel(pred_next_state, self.actor(pred_next_state.detach())).clamp(self.obs_lower_bound,self.obs_upper_bound)\n","                # third term\n","                # r ( s_{t+1}, s_{t+2}, actor(s_{t+1}))\n","                pred_next_next_r = self.rmodel(pred_next_state.detach() ,pred_next_next_state.detach(), self.actor(pred_next_state.detach()))\n","\n","                # compute last term\n","                action_next_next = self.actor(pred_next_next_state.detach())\n","                last_loss = self.critic(pred_next_next_state.detach(), action_next_next)[0]\n","                final_loss = -first_loss - weight * torch.mean( pred_next_r + self.gamma * pred_next_next_r + self.gamma * self.gamma * last_loss )\n","\n","              else:\n","                final_loss = -first_loss\n","\n","              # Optimize the actor\n","              if totrain == 1:\n","                self.crt_opt.zero_grad()\n","                self.sys_opt.zero_grad()\n","                self.r_opt.zero_grad()\n","\n","                self.act_opt.zero_grad()\n","                final_loss.backward()\n","                self.act_opt.step()\n","\n","              self.soft_update_target(self.actor, self.actor_target)\n","              self.soft_update_target(self.critic, self.critic_target)\n","\n","\n","\n","\n","    def soft_update_target(self,local_model,target_model):\n","        \"\"\"Soft update model parameters.\n","        Formula: θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Args:\n","            local_model: PyTorch model (weights will be copied from)\n","            target_model: PyTorch model (weights will be copied to)\n","            tau (float): interpolation parameter\n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n","\n","    def policy(self,state):\n","        \"\"\"select action based on ACTOR but with noise added\"\"\"\n","\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        self.actor.eval()\n","        with torch.no_grad():\n","            actions = self.actor(state).cpu().data.numpy()\n","        self.actor.train()\n","        # Adding noise to action\n","        shift_action = np.random.normal(0, self.std_noise, size=self.env.action_space.shape[0])\n","        sampled_actions = (actions + shift_action)\n","        # We make sure action is within bounds\n","        legal_action = np.clip(sampled_actions,self.lower_bound,self.upper_bound)\n","        return np.squeeze(legal_action)\n","\n","\n","    def select_action(self,state):\n","        \"\"\"select action based on ACTOR\"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            actions = self.actor_target(state).cpu().data.numpy()\n","        return np.squeeze(actions)\n"]},{"cell_type":"code","source":["import time\n","import os\n","gym.logger.set_level(40)\n","max_steps = 3000\n","falling_down = 0\n","\n","\n","if __name__ == '__main__':\n","    env = gym.make('BipedalWalkerHardcore-v3')\n","    agent = Agent(batch_size = 100) # Use TD3 for example\n","    total_episodes = 10000\n","    start_timestep=0            # time_step to select action based on Actor\n","    time_start = time.time()        # Init start time\n","    ep_reward_list = []\n","    avg_reward_list = []\n","    total_timesteps = 0\n","    save_time = 0\n","    expcount = 0\n","    totrain = 0\n","\n","    best_reward = -9999\n","\n","    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"rl_coding_proj\"\n","    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","    actor_path = \"actor_512.pth\"\n","    path = os.path.join(GOOGLE_DRIVE_PATH, actor_path)\n","\n","    for ep in range(total_episodes):\n","        state = env.reset()\n","        episodic_reward = 0\n","        timestep = 0\n","        temp_replay_buffer = []\n","        for st in range(max_steps):\n","            # Select action randomly or according to policy\n","            if total_timesteps < start_timestep:\n","                action = env.action_space.sample()\n","            else:\n","                action = agent.policy(state)\n","            # Recieve state and reward from environment.\n","            next_state, reward, done, info = env.step(action)\n","            # Change original reward from -100 to -5 and 5*reward for other values\n","            episodic_reward += reward\n","            if reward == -100:\n","                add_reward = -1\n","                reward = -5\n","                falling_down += 1 # check whether the bipedal falls down or not\n","                expcount += 1\n","            else:\n","                add_reward = 0\n","                reward = 5 * reward\n","\n","            temp_replay_buffer.append((state, action, reward, next_state, done))\n","\n","            # End this episode when `done` is True\n","            # Replay buffer\n","            if done:\n","                if add_reward == -1 or episodic_reward < 250:\n","                    totrain = 1\n","                    for temp in temp_replay_buffer:\n","                        agent.add_to_replay_memory(temp, agent.replay_memory_buffer)\n","                elif expcount > 0 and np.random.rand() > 0.5:\n","                    totrain = 1\n","                    expcount -= 10\n","                    for temp in temp_replay_buffer:\n","                        agent.add_to_replay_memory(temp, agent.replay_memory_buffer)\n","                break\n","            state = next_state\n","            timestep += 1\n","            total_timesteps += 1\n","\n","        ep_reward_list.append(episodic_reward)\n","        # Mean of last 100 episodes\n","        avg_reward = np.mean(ep_reward_list[-100:])\n","        avg_reward_list.append(avg_reward)\n","        s = (int)(time.time() - time_start)\n","\n","        #Training agent only when new experiences are added to the replay buffer\n","        #If totrain == 1 we update actor network else we only update critic network\n","        weight =  (1 - np.clip(np.mean(ep_reward_list[-100:])/320, 0, 1)) * 1.5\n","        #print(\"weight: \", weight)\n","\n","        if totrain == 1:\n","            agent.learn_and_update_weights_by_replay(timestep, weight, totrain)\n","        else:\n","            agent.learn_and_update_weights_by_replay(100, weight, totrain)\n","        totrain = 0\n","\n","        print('Ep. {}, Timestep {},  Ep.Timesteps {}, Episode Reward: {:.2f}, Moving Avg.Reward: {:.2f}, Time: {:02}:{:02}:{:02}'.format(ep, total_timesteps, timestep,\n","                      episodic_reward, avg_reward, s//3600, s%3600//60, s%60))\n","        if avg_reward > 300:\n","\n","          if avg_reward > best_reward:\n","            best_reward = avg_reward\n","            torch.save(agent.actor.to(\"cpu\").state_dict(), path)\n","            agent.actor.to(\"cuda\")\n","            print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","            if avg_reward > 310:\n","              break\n","\n","\n",""],"metadata":{"id":"vVTyuh0EqCBV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","gym.logger.set_level(40)\n","max_steps = 3000\n","falling_down = 0\n","\n","\n","if __name__ == '__main__':\n","    env = gym.make('BipedalWalkerHardcore-v3')\n","    agent = Agent()\n","    total_episodes = 1000\n","    start_timestep=0            # time_step to select action based on Actor\n","    time_start = time.time()        # Init start time\n","    ep_reward_list = []\n","    avg_reward_list = []\n","    total_timesteps = 0\n","    save_time = 0\n","    expcount = 0\n","    totrain = 0\n","\n","    for ep in range(total_episodes):\n","        state = env.reset()\n","        episodic_reward = 0\n","        timestep = 0\n","        temp_replay_buffer = []\n","        for st in range(max_steps):\n","\n","            action = agent.select_action(state)\n","            # Recieve state and reward from environment.\n","            next_state, reward, done, info = env.step(action)\n","            episodic_reward += reward\n","\n","            # temp_replay_buffer.append((state, action, reward, next_state, done))\n","\n","            if done:\n","              break\n","\n","            state = next_state\n","            timestep += 1\n","            total_timesteps += 1\n","\n","        ep_reward_list.append(episodic_reward)\n","        avg_reward = np.mean(ep_reward_list[-100:])\n","        avg_reward_list.append(avg_reward)\n","        s = (int)(time.time() - time_start)\n","\n","        print('Ep. {}, Timestep {},  Ep.Timesteps {}, Episode Reward: {:.2f}, Moving Avg.Reward: {:.2f}, Time: {:02}:{:02}:{:02}'.format(ep, total_timesteps, timestep,\n","                      episodic_reward, avg_reward, s//3600, s%3600//60, s%60))\n","\n",""],"metadata":{"id":"naxU79lRixRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save\n","import os\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"rl_coding_proj\"\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","actor_path = \"actor.pth\"\n","path = os.path.join(GOOGLE_DRIVE_PATH, actor_path)\n","torch.save(agent.actor.to(\"cpu\").state_dict(), path)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"metadata":{"id":"RosAJ5m1UvyA"},"execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Create Assignment","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}