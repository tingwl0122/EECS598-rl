{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar x /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/ROMS"],"metadata":{"id":"5ke5LjFVQbuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q ale-py\n","!apt-get install x11-utils > /dev/null 2>&1\n","!pip install pyglet > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install -U colabgymrender"],"metadata":{"id":"JCrHxydPjZpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import deque\n","import numpy as np\n","import gym\n","from gym import spaces, wrappers\n","import cv2\n","cv2.ocl.setUseOpenCL(False)\n","\n","class NoopResetEnv(gym.Wrapper):\n","    def __init__(self, env, noop_max=30):\n","        gym.Wrapper.__init__(self, env)\n","        self.noop_max = noop_max\n","        self.override_num_noops = None\n","        self.noop_action = 0\n","        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        if self.override_num_noops is not None:\n","            noops = self.override_num_noops\n","        else:\n","            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n","        assert noops > 0\n","        obs = None\n","        for _ in range(noops):\n","            obs, _, done, _ = self.env.step(self.noop_action)\n","            if done:\n","                obs = self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, action):\n","        return self.env.step(action)\n","\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        gym.Wrapper.__init__(self, env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, action):\n","        return self.env.step(action)\n","\n","\n","class EpisodicLifeEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        gym.Wrapper.__init__(self, env)\n","        self.lives = 0\n","        self.was_real_done = True\n","\n","    def step(self, action):\n","        obs, reward, done, info = self.env.step(action)\n","        self.was_real_done = done\n","        # check current lives, make loss of life terminal,\n","        # then update lives to handle bonus lives\n","        lives = self.env.unwrapped.ale.lives()\n","        if 0 < lives < self.lives:\n","            # for Qbert sometimes we stay in lives == 0 condtion for a few\n","            # frames so its important to keep lives > 0, so that we only reset\n","            # once the environment advertises done.\n","            done = True\n","        self.lives = lives\n","        return obs, reward, done, info\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Calls the Gym environment reset, only when lives are exhausted.\n","        This way all states are still reachable even though lives are episodic,\n","        and the learner need not know about any of this behind-the-scenes.\n","        Args:\n","            Extra keywords passed to env.reset() call\n","        Return:\n","            ([int] or [float]) the first observation of the environment\n","        \"\"\"\n","        if self.was_real_done:\n","            obs = self.env.reset(**kwargs)\n","        else:\n","            # no-op step to advance from terminal/lost life state\n","            obs, _, _, _ = self.env.step(0)\n","        self.lives = self.env.unwrapped.ale.lives()\n","        return obs\n","\n","class MaxAndSkipEnv(gym.Wrapper):\n","    def __init__(self, env, skip=4):\n","        \"\"\"\n","        Return only every `skip`-th frame (frameskipping)\n","        Args:\n","            env: (Gym Environment) the environment\n","            skip: (int) number of `skip`-th frame\n","        Return:\n","            (Gym Environment) the environment with only `skip`-th frame\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        # most recent raw observations (for max pooling across time steps)\n","        self._obs_buffer = np.zeros(\n","            (2,)+env.observation_space.shape,\n","            dtype=env.observation_space.dtype)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"\n","        Step the environment with the given action\n","        Repeat action, sum reward, and max over last observations.\n","        Args:\n","            action: ([int] or [float]) the action\n","        Return:\n","            ([int] or [float], [float], [bool], dict) observation, reward,\n","                 done, information\n","        \"\"\"\n","        total_reward = 0.0\n","        done = None\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            if i == self._skip - 2:\n","                self._obs_buffer[0] = obs\n","            if i == self._skip - 1:\n","                self._obs_buffer[1] = obs\n","            total_reward += reward\n","            if done:\n","                break\n","        max_frame = self._obs_buffer.max(axis=0)\n","\n","        return max_frame, total_reward, done, info\n","\n","    def reset(self, **kwargs):\n","        return self.env.reset(**kwargs)\n","\n","\n","class ClipRewardEnv(gym.RewardWrapper):\n","    def __init__(self, env):\n","        \"\"\"\n","        clips the reward to {+1, 0, -1} by its sign.\n","        :param env: (Gym Environment) the environment\n","        \"\"\"\n","        gym.RewardWrapper.__init__(self, env)\n","\n","    def reward(self, reward):\n","        \"\"\"\n","        Bin reward to {+1, 0, -1} by its sign.\n","        \"\"\"\n","        return np.sign(reward)\n","\n","\n","class WarpFramePyTorch(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        \"\"\"\n","        Warp frames to 84x84 as done in the Nature paper and later work.\n","        Args:\n","            env: (Gym Environment) the environment\n","        \"\"\"\n","        gym.ObservationWrapper.__init__(self, env)\n","        self.width = 84\n","        self.height = 84\n","        self.observation_space = spaces.Box(\n","            low=0, high=255, shape=(1, self.height, self.width),\n","            dtype=env.observation_space.dtype)\n","\n","    def observation(self, frame):\n","        \"\"\"\n","        returns the current observation from a frame\n","        Args:\n","            frame: ([int] or [float]) environment frame\n","\n","        Return:\n","            ([int] or [float]) the observation\n","        \"\"\"\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","        frame = cv2.resize(\n","            frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n","        return frame[None, :, :]\n","\n","\n","class FrameStackPyTorch(gym.Wrapper):\n","    def __init__(self, env, n_frames):\n","        \"\"\"Stack n_frames last frames.\n","        Args:\n","            env: (Gym Environment) the environment\n","            n_frames: (int) the number of frames to stack\n","        \"\"\"\n","        assert env.observation_space.dtype == np.uint8\n","\n","        gym.Wrapper.__init__(self, env)\n","        self.n_frames = n_frames\n","        self.frames = deque([], maxlen=n_frames)\n","        shp = env.observation_space.shape\n","\n","        self.observation_space = spaces.Box(\n","            low=np.min(env.observation_space.low),\n","            high=np.max(env.observation_space.high),\n","            shape=(shp[0] * n_frames, shp[1], shp[2]),\n","            dtype=env.observation_space.dtype)\n","\n","    def reset(self):\n","        obs = self.env.reset()\n","        for _ in range(self.n_frames):\n","            self.frames.append(obs)\n","        return self._get_ob()\n","\n","    def step(self, action):\n","        obs, reward, done, info = self.env.step(action)\n","        self.frames.append(obs)\n","        return self._get_ob(), reward, done, info\n","\n","    def _get_ob(self):\n","        assert len(self.frames) == self.n_frames\n","        return LazyFrames(list(self.frames))\n","\n","\n","class ScaledFloatFrame(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        gym.ObservationWrapper.__init__(self, env)\n","        self.observation_space = spaces.Box(\n","            low=0, high=1.0, shape=env.observation_space.shape,\n","            dtype=np.float32)\n","\n","    def observation(self, observation):\n","        # careful! This undoes the memory optimization, use\n","        # with smaller replay buffers only.\n","        return np.array(observation).astype(np.float32) / 255.0\n","\n","\n","class LazyFrames(object):\n","    def __init__(self, frames):\n","        self._frames = frames\n","        self.dtype = frames[0].dtype\n","\n","    def _force(self):\n","        return np.concatenate(\n","            np.array(self._frames, dtype=self.dtype), axis=0)\n","\n","    def __array__(self, dtype=None):\n","        out = self._force()\n","        if dtype is not None:\n","            out = out.astype(dtype)\n","        return out\n","\n","    def __len__(self):\n","        return len(self._force())\n","\n","    def __getitem__(self, i):\n","        return self._force()[i]\n","\n","\n","def make_atari(env_id):\n","    \"\"\"\n","    Create a wrapped atari envrionment\n","    Args:\n","        env_id: (str) the environment ID\n","    Return:\n","        (Gym Environment) the wrapped atari environment\n","    \"\"\"\n","    env = gym.make(env_id)\n","    assert 'NoFrameskip' in env.spec.id\n","    env = NoopResetEnv(env, noop_max=30)\n","    env = MaxAndSkipEnv(env, skip=4)\n","    return env\n","\n","\n","def wrap_deepmind_pytorch(env, episode_life=True, clip_rewards=True,\n","                          frame_stack=True, scale=False):\n","    \"\"\"\n","    Configure environment for DeepMind-style Atari.\n","    Args:\n","        env: (Gym Environment) the atari environment\n","        episode_life: (bool) wrap the episode life wrapper\n","        clip_rewards: (bool) wrap the reward clipping wrapper\n","        frame_stack: (bool) wrap the frame stacking wrapper\n","        scale: (bool) wrap the scaling observation wrapper\n","    Return:\n","        (Gym Environment) the wrapped atari environment\n","    \"\"\"\n","    if episode_life:\n","        env = EpisodicLifeEnv(env)\n","    if 'FIRE' in env.unwrapped.get_action_meanings():\n","        env = FireResetEnv(env)\n","    env = WarpFramePyTorch(env)\n","    if clip_rewards:\n","        env = ClipRewardEnv(env)\n","    if scale:\n","        env = ScaledFloatFrame(env)\n","    if frame_stack:\n","        env = FrameStackPyTorch(env, 4)\n","    return env\n","\n","\n","def make_pytorch_env(env_id, episode_life=True, clip_rewards=True,\n","                     frame_stack=True, scale=False):\n","    env = make_atari(env_id)\n","    env = wrap_deepmind_pytorch(\n","        env, episode_life, clip_rewards, frame_stack, scale)\n","    return env\n","\n","\n","def wrap_monitor(env, log_dir):\n","    env = wrappers.Monitor(\n","        env, log_dir, video_callable=lambda x: True)\n","    return env"],"metadata":{"id":"zG3c2LJtK03U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import deque\n","import numpy as np\n","import torch\n","import operator\n","\n","class MultiStepBuff:\n","\n","    def __init__(self, maxlen=3):\n","        super(MultiStepBuff, self).__init__()\n","        self.maxlen = int(maxlen)\n","        self.reset()\n","\n","    def append(self, state, action, reward):\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.rewards.append(reward)\n","\n","    def get(self, gamma=0.99):\n","        assert len(self.rewards) > 0\n","        state = self.states.popleft()\n","        action = self.actions.popleft()\n","        reward = self._nstep_return(gamma)\n","        return state, action, reward\n","\n","    def _nstep_return(self, gamma):\n","        r = np.sum([r * (gamma ** i) for i, r in enumerate(self.rewards)])\n","        self.rewards.popleft()\n","        return r\n","\n","    def reset(self):\n","        # Buffer to store n-step transitions.\n","        self.states = deque(maxlen=self.maxlen)\n","        self.actions = deque(maxlen=self.maxlen)\n","        self.rewards = deque(maxlen=self.maxlen)\n","\n","    def is_empty(self):\n","        return len(self.rewards) == 0\n","\n","    def is_full(self):\n","        return len(self.rewards) == self.maxlen\n","\n","    def __len__(self):\n","        return len(self.rewards)\n","\n","\n","class LazyMemory(dict):\n","    state_keys = ['state', 'next_state']\n","    np_keys = ['action', 'reward', 'done']\n","    keys = state_keys + np_keys\n","\n","    def __init__(self, capacity, state_shape, device):\n","        super(LazyMemory, self).__init__()\n","        self.capacity = int(capacity)\n","        self.state_shape = state_shape\n","        self.device = device\n","        self.reset()\n","\n","    def reset(self):\n","        self['state'] = []\n","        self['next_state'] = []\n","\n","        self['action'] = np.empty((self.capacity, 1), dtype=np.int64)\n","        self['reward'] = np.empty((self.capacity, 1), dtype=np.float32)\n","        self['done'] = np.empty((self.capacity, 1), dtype=np.float32)\n","\n","        self._n = 0\n","        self._p = 0\n","\n","    def append(self, state, action, reward, next_state, done,\n","               episode_done=None):\n","        self._append(state, action, reward, next_state, done)\n","\n","    def _append(self, state, action, reward, next_state, done):\n","        self['state'].append(state)\n","        self['next_state'].append(next_state)\n","        self['action'][self._p] = action\n","        self['reward'][self._p] = reward\n","        self['done'][self._p] = done\n","\n","        self._n = min(self._n + 1, self.capacity)\n","        self._p = (self._p + 1) % self.capacity\n","\n","        self.truncate()\n","\n","    def truncate(self):\n","        while len(self) > self.capacity:\n","            del self['state'][0]\n","            del self['next_state'][0]\n","\n","    def sample(self, batch_size):\n","        indices = np.random.randint(low=0, high=len(self), size=batch_size)\n","        return self._sample(indices, batch_size)\n","\n","    def _sample(self, indices, batch_size):\n","        bias = -self._p if self._n == self.capacity else 0\n","\n","        states = np.empty(\n","            (batch_size, *self.state_shape), dtype=np.uint8)\n","        next_states = np.empty(\n","            (batch_size, *self.state_shape), dtype=np.uint8)\n","\n","        for i, index in enumerate(indices):\n","            _index = np.mod(index+bias, self.capacity)\n","            states[i, ...] = self['state'][_index]\n","            next_states[i, ...] = self['next_state'][_index]\n","\n","        states = torch.ByteTensor(states).to(self.device).float() / 255.\n","        next_states = torch.ByteTensor(\n","            next_states).to(self.device).float() / 255.\n","        actions = torch.LongTensor(self['action'][indices]).to(self.device)\n","        rewards = torch.FloatTensor(self['reward'][indices]).to(self.device)\n","        dones = torch.FloatTensor(self['done'][indices]).to(self.device)\n","\n","        return states, actions, rewards, next_states, dones\n","\n","    def __len__(self):\n","        return len(self['state'])\n","\n","    def get(self):\n","        return dict(self)\n","\n","    def load(self, memory):\n","        for key in self.state_keys:\n","            self[key].extend(memory[key])\n","\n","        num_data = len(memory['state'])\n","        if self._p + num_data <= self.capacity:\n","            for key in self.np_keys:\n","                self[key][self._p:self._p+num_data] = memory[key]\n","        else:\n","            mid_index = self.capacity - self._p\n","            end_index = num_data - mid_index\n","            for key in self.np_keys:\n","                self[key][self._p:] = memory[key][:mid_index]\n","                self[key][:end_index] = memory[key][mid_index:]\n","\n","        self._n = min(self._n + num_data, self.capacity)\n","        self._p = (self._p + num_data) % self.capacity\n","        self.truncate()\n","        assert self._n == len(self)\n","\n","\n","class LazyMultiStepMemory(LazyMemory):\n","\n","    def __init__(self, capacity, state_shape, device, gamma=0.99,\n","                 multi_step=3):\n","        super(LazyMultiStepMemory, self).__init__(\n","            capacity, state_shape, device)\n","\n","        self.gamma = gamma\n","        self.multi_step = int(multi_step)\n","        if self.multi_step != 1:\n","            self.buff = MultiStepBuff(maxlen=self.multi_step)\n","\n","    def append(self, state, action, reward, next_state, done):\n","        if self.multi_step != 1:\n","            self.buff.append(state, action, reward)\n","\n","            if self.buff.is_full():\n","                state, action, reward = self.buff.get(self.gamma)\n","                self._append(state, action, reward, next_state, done)\n","\n","            if done:\n","                while not self.buff.is_empty():\n","                    state, action, reward = self.buff.get(self.gamma)\n","                    self._append(state, action, reward, next_state, done)\n","        else:\n","            self._append(state, action, reward, next_state, done)\n","import numpy as np\n","import torch"],"metadata":{"id":"ZnC6hpInQb-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LinearAnneaer:\n","    def __init__(self, start_value, end_value, num_steps):\n","        assert num_steps > 0 and isinstance(num_steps, int)\n","\n","        self.steps = 0\n","        self.start_value = start_value\n","        self.end_value = end_value\n","        self.num_steps = num_steps\n","\n","        self.a = (self.end_value - self.start_value) / self.num_steps\n","        self.b = self.start_value\n","\n","    def step(self):\n","        self.steps = min(self.num_steps, self.steps + 1)\n","\n","    def get(self):\n","        assert 0 < self.steps <= self.num_steps\n","        return self.a * self.steps + self.b\n","\n","def disable_gradients(network):\n","    # Disable calculations of gradients.\n","    for param in network.parameters():\n","        param.requires_grad = False"],"metadata":{"id":"S07tTOlItIuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from copy import copy\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","def initialize_weights_xavier(m, gain=1.0):\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight, gain=gain)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","\n","def initialize_weights_he(m):\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        torch.nn.init.kaiming_uniform_(m.weight)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","# The input image size is 7x7x64\n","# The DQNbase is used for feature extraction.\n","class DQNBase(nn.Module):\n","    def __init__(self, num_channels, embedding_dim=7*7*64):\n","        super(DQNBase, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            Flatten(),\n","        ).apply(initialize_weights_he)\n","        self.embedding_dim = embedding_dim\n","\n","    def forward(self, states):\n","        batch_size = states.shape[0]\n","\n","        # Calculate embeddings of states.\n","        state_embedding = self.net(states)\n","        assert state_embedding.shape == (batch_size, self.embedding_dim)\n","\n","        return state_embedding\n","\n","class BaseModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","class DQN(BaseModel):\n","    def __init__(self, num_channels, num_actions, embedding_dim=7*7*64):\n","        super(DQN, self).__init__()\n","        # Feature extractor of DQN.\n","        self.feature = DQNBase(num_channels=num_channels)\n","        # DQN network.\n","        linear = nn.Linear\n","        self.q_net = nn.Sequential(\n","                linear(embedding_dim, 512),\n","                nn.ReLU(),\n","                linear(512, num_actions),\n","            )\n","        self.num_channels = num_channels\n","        self.num_actions = num_actions\n","        self.embedding_dim = embedding_dim\n","\n","    def forward(self, states=None):\n","        batch_size = states.shape[0]\n","        state_embeddings = self.feature(states)\n","        outputs = self.q_net(state_embeddings).view(batch_size,self.num_actions)\n","        return outputs\n","\n","\n","class DuelNoisyQ(nn.Module):\n","    def __init__(self, num_channels, num_actions, embedding_dim=7*7*64, fc1_units=256, fc2_units=256, num_atoms=51):\n","        super(DuelNoisyQ, self).__init__()\n","        # self.seed = torch.manual_seed(seed)\n","\n","        # Feature extractor of DQN.\n","        self.feature = DQNBase(num_channels=num_channels)\n","\n","        # set advantage layer\n","        self.advantage_hidden_layer = NoisyLinear(embedding_dim, fc1_units)\n","        self.advantage_layer = NoisyLinear(fc1_units, num_actions * num_atoms)\n","\n","        # set value layer\n","        self.value_hidden_layer = NoisyLinear(embedding_dim, fc1_units)\n","        self.value_layer = NoisyLinear(fc1_units, num_atoms)\n","\n","        # V architecture\n","        self.v_net = nn.Sequential(\n","                # nn.Linear(embedding_dim, fc1_units),\n","                self.value_hidden_layer,\n","                # nn.ReLU(),\n","                # nn.Linear(fc1_units, fc2_units),\n","                nn.ReLU(),\n","                self.value_layer\n","            )\n","\n","\n","        # Advantage architecture\n","        self.adv_net = nn.Sequential(\n","                # nn.Linear(embedding_dim, fc1_units),\n","                self.advantage_hidden_layer,\n","                # nn.ReLU(),\n","                # nn.Linear(fc1_units, fc2_units),\n","                nn.ReLU(),\n","                self.advantage_layer\n","            )\n","\n","\n","        self.num_channels = num_channels\n","        self.num_actions = num_actions\n","        self.embedding_dim = embedding_dim\n","    def forward(self, states=None):\n","        feature = self.feature(states)\n","        advantage = self.adv_net(feature).view(-1, self.num_actions, self.atom_size)\n","        value     = self.v_net(feature).view(-1, 1, self.atom_size)\n","\n","        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n","\n","        dist = torch.softmax(q_atoms, dim=-1)\n","        q = torch.sum(dist * self.support, dim=2)\n","\n","        return q\n","\n","    def reset_noise(self):\n","        \"\"\"Reset all noisy layers.\"\"\"\n","        self.advantage_hidden_layer.reset_noise()\n","        self.advantage_layer.reset_noise()\n","        self.value_hidden_layer.reset_noise()\n","        self.value_layer.reset_noise()\n","\n","\n","class NoisyLinear(nn.Module):\n","    def __init__(self, in_features, out_features, sigma=0.5):\n","        super(NoisyLinear, self).__init__()\n","\n","        # Learnable parameters.\n","        self.mu_W = nn.Parameter(\n","            torch.FloatTensor(out_features, in_features))\n","        self.sigma_W = nn.Parameter(\n","            torch.FloatTensor(out_features, in_features))\n","        self.mu_bias = nn.Parameter(torch.FloatTensor(out_features))\n","        self.sigma_bias = nn.Parameter(torch.FloatTensor(out_features))\n","\n","        # Factorized noise parameters.\n","        self.register_buffer('eps_p', torch.FloatTensor(in_features))\n","        self.register_buffer('eps_q', torch.FloatTensor(out_features))\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.sigma = sigma\n","\n","        self.reset()\n","        self.sample()\n","\n","    def reset(self):\n","        bound = 1 / np.sqrt(self.in_features)\n","        self.mu_W.data.uniform_(-bound, bound)\n","        self.mu_bias.data.uniform_(-bound, bound)\n","        self.sigma_W.data.fill_(self.sigma / np.sqrt(self.in_features))\n","        self.sigma_bias.data.fill_(self.sigma / np.sqrt(self.out_features))\n","\n","    def f(self, x):\n","        return x.normal_().sign().mul(x.abs().sqrt())\n","\n","    def sample(self):\n","        self.eps_p.copy_(self.f(self.eps_p))\n","        self.eps_q.copy_(self.f(self.eps_q))\n","\n","    def forward(self, x):\n","        if self.training:\n","            weight = self.mu_W + self.sigma_W * self.eps_q.ger(self.eps_p)\n","            bias = self.mu_bias + self.sigma_bias * self.eps_q.clone()\n","        else:\n","            weight = self.mu_W\n","            bias = self.mu_bias\n","\n","        return F.linear(x, weight, bias)\n"],"metadata":{"id":"YM46XE5WqNLi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QRDQN(nn.Module):\n","    def __init__(self, num_channels, num_actions, embedding_dim=7*7*64, num_quants=50):\n","        super(QRDQN, self).__init__()\n","\n","\n","        self.features = DQNBase(num_channels=num_channels)\n","        self.advantage_net = nn.Sequential(\n","                nn.Linear(embedding_dim, 256),\n","                nn.ReLU(),\n","                nn.Linear(256, num_actions * num_quants),\n","            )\n","        self.value_net = nn.Sequential(\n","                nn.Linear(embedding_dim, 256),\n","                nn.ReLU(),\n","                nn.Linear(256, num_quants),\n","            )\n","\n","        self.embedding_dim  = embedding_dim\n","        self.num_actions = num_actions\n","        self.num_quants  = num_quants\n","\n","\n","\n","    def forward(self, states=None):\n","\n","        batch_size = states.shape[0]\n","\n","        feature = self.features(states)\n","\n","        advantages = self.advantage_net(feature).view(batch_size, self.num_quants, self.num_actions)\n","        baselines = self.value_net(feature).view(batch_size, self.num_quants, 1)\n","        quantiles = baselines + advantages - advantages.mean(dim=2, keepdim=True)\n","\n","        assert quantiles.shape == (batch_size, self.num_quants, self.num_actions)\n","\n","        return quantiles\n","\n","    def calculate_q(self, states=None):\n","\n","        batch_size = states.shape[0]\n","\n","        # Calculate quantiles.\n","        quantiles = self(states=states)\n","\n","        # Calculate expectations of value distributions.\n","        q = quantiles.mean(dim=1)\n","        assert q.shape == (batch_size, self.num_actions)\n","\n","        return q\n","\n"],"metadata":{"id":"zArzGH1ptQXi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from abc import ABC, abstractmethod\n","import os\n","from torch.optim import Adam\n","from collections import deque\n","import random\n","\n","\n","class BaseAgent(ABC):\n","    def __init__(self, env, test_env, num_steps=5*(10**7),\n","                 batch_size=32, memory_size=10**6, gamma=0.99, multi_step=1,\n","                 update_interval=4, target_update_interval=10000,\n","                 start_steps=50000, epsilon_train=0.01, epsilon_eval=0.001,\n","                 epsilon_decay_steps=250000, double_q_learning=False,\n","                 dueling_net=False, noisy_net=False, use_per=False,\n","                 log_interval=100, eval_interval=250000, num_eval_steps=125000,\n","                 max_episode_steps=27000, grad_cliping=5.0, cuda=True, seed=0, num_quants=50):\n","        self.env = env\n","        self.test_env = test_env\n","\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        self.env.seed(seed)\n","        self.test_env.seed(2**31-1-seed)\n","        self.device = torch.device(\n","            \"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")\n","        self.online_net = None\n","        self.target_net = None\n","        self.memory = LazyMultiStepMemory(\n","                memory_size, self.env.observation_space.shape,\n","                self.device, gamma, multi_step)\n","        self.steps = 0\n","        self.learning_steps = 0\n","        self.episodes = 0\n","        self.best_eval_score = -np.inf # used to record best evaluation reward\n","        self.num_actions = self.env.action_space.n  # dimension of action space\n","        self.num_steps = num_steps  # total training number steps\n","        self.batch_size = batch_size # Batch size when training neural networks\n","        self.eval_interval = eval_interval # The frequency of evaluating agents\n","        self.num_eval_steps = num_eval_steps # Evaluation steps\n","        self.gamma = gamma ** multi_step # Discount factor\n","        self.start_steps = start_steps\n","        self.epsilon_train = LinearAnneaer(\n","            1.0, epsilon_train, epsilon_decay_steps)\n","        self.epsilon_eval = epsilon_eval # minimum epsion when evaluating agents\n","        self.update_interval = update_interval\n","        self.target_update_interval = target_update_interval # how many steps are needed to update target network\n","        self.max_episode_steps = max_episode_steps # max episode steps\n","        self.grad_cliping = grad_cliping # clip gradient\n","        self.num_quants = num_quants\n","\n","\n","    def run(self):\n","        while True:\n","            self.train_episode()\n","            if self.steps > self.num_steps:\n","                break\n","\n","    def is_update(self):\n","        return self.steps % self.update_interval == 0\\\n","            and self.steps >= self.start_steps\n","\n","    def is_random(self, eval=False):\n","        # Use e-greedy for evaluation.\n","        if self.steps < self.start_steps:\n","            return True\n","        if eval:\n","            return np.random.rand() < self.epsilon_eval\n","        return np.random.rand() < self.epsilon_train.get()\n","\n","    def update_target(self):\n","        self.target_net.load_state_dict(\n","            self.online_net.state_dict())\n","\n","    def explore(self):\n","        # Act with randomness.\n","        action = self.env.action_space.sample()\n","        return action\n","\n","    def select_action(self, state):\n","        # Act without randomness.\n","        state = torch.ByteTensor(\n","            state).unsqueeze(0).to(self.device).float() / 255.\n","        with torch.no_grad():\n","            action = self.online_net.calculate_q(states=state).argmax().item()\n","        return action\n","\n","    @abstractmethod\n","    def learn(self):\n","        pass\n","    def train_episode(self):\n","        self.online_net.train()\n","        self.target_net.train()\n","\n","        self.episodes += 1\n","        episode_return = 0.\n","        episode_steps = 0\n","\n","        done = False\n","        state = self.env.reset()\n","        ep_reward_list = []\n","        avg_reward_list = []\n","\n","        while (not done) and episode_steps <= self.max_episode_steps:\n","            if self.is_random(eval=False):\n","                action = self.explore()\n","            else:\n","                action = self.select_action(state)\n","            next_state, reward, done, _ = self.env.step(action)\n","            self.memory.append(state, action, reward, next_state, done)\n","            self.steps += 1\n","            episode_steps += 1\n","            episode_return += reward\n","            state = next_state\n","\n","            self.train_step_interval()\n","\n","        avg_return = np.mean(ep_reward_list[-100:])\n","\n","        print(f'Episode: {self.episodes:<4} '\n","              f'episode steps: {episode_steps:<4} '\n","              f'total steps: {self.steps:<4}  '\n","              f'return: {episode_return:<5.1f}'\n","              )\n","        ep_reward_list.append(episode_return)\n","        avg_reward_list.append(avg_return)\n","\n","\n","\n","    def train_step_interval(self):\n","        self.epsilon_train.step()\n","\n","        if self.steps % self.target_update_interval == 0:\n","            self.update_target()\n","\n","        if self.is_update():\n","            self.learn()\n","\n","        if self.steps % self.eval_interval == 0:\n","            self.evaluate()\n","            self.online_net.train()\n","\n","    def evaluate(self):\n","        self.online_net.eval()\n","        num_episodes = 0\n","        num_steps = 0\n","        total_return = 0.0\n","\n","        while True:\n","            state = self.test_env.reset()\n","            episode_steps = 0\n","            episode_return = 0.0\n","            done = False\n","            while (not done) and episode_steps <= self.max_episode_steps:\n","                if self.is_random(eval=True):\n","                    action = self.explore()\n","                else:\n","                    action = self.select_action(state)\n","\n","                next_state, reward, done, _ = self.test_env.step(action)\n","                num_steps += 1\n","                episode_steps += 1\n","                episode_return += reward\n","                state = next_state\n","\n","            num_episodes += 1\n","            total_return += episode_return\n","\n","            if num_steps > self.num_eval_steps:\n","                break\n","\n","        mean_return = total_return / num_episodes\n","\n","        if mean_return > self.best_eval_score:\n","            self.best_eval_score = mean_return\n","\n","        print('-' * 60)\n","        print(f'Num steps: {self.steps:<5}  '\n","              f'return: {mean_return:<5.1f}')\n","        print('-' * 60)\n","\n","    def __del__(self):\n","        self.env.close()\n","        self.test_env.close()\n","\n","\n","def calculate_huber_loss(td_errors, kappa=1.0):\n","    return torch.where(\n","        td_errors.abs() <= kappa,\n","        0.5 * td_errors.pow(2),\n","        kappa * (td_errors.abs() - 0.5 * kappa))\n","\n","def calculate_quantile_huber_loss(td_errors, taus, weights=None, kappa=1.0):\n","    assert not taus.requires_grad\n","    batch_size, N, N_dash = td_errors.shape\n","\n","    # Calculate huber loss element-wisely.\n","    element_wise_huber_loss = calculate_huber_loss(td_errors, kappa)\n","    assert element_wise_huber_loss.shape == (\n","        batch_size, N, N_dash)\n","\n","    # Calculate quantile huber loss element-wisely.\n","    element_wise_quantile_huber_loss = torch.abs(\n","        taus[..., None] - (td_errors.detach() < 0).float()\n","        ) * element_wise_huber_loss / kappa\n","    assert element_wise_quantile_huber_loss.shape == (\n","        batch_size, N, N_dash)\n","\n","    # Quantile huber loss.\n","    batch_quantile_huber_loss = element_wise_quantile_huber_loss.sum(\n","        dim=1).mean(dim=1, keepdim=True)\n","    assert batch_quantile_huber_loss.shape == (batch_size, 1)\n","\n","    if weights is not None:\n","        quantile_huber_loss = (batch_quantile_huber_loss * weights).mean()\n","    else:\n","        quantile_huber_loss = batch_quantile_huber_loss.mean()\n","\n","    return quantile_huber_loss\n","\n","def evaluate_quantile_at_action(s_quantiles, actions):\n","    assert s_quantiles.shape[0] == actions.shape[0]\n","\n","    batch_size = s_quantiles.shape[0]\n","    N = s_quantiles.shape[1]\n","\n","    # Expand actions into (batch_size, N, 1).\n","    action_index = actions[..., None].expand(batch_size, N, 1)\n","\n","    # Calculate quantile values at specified actions.\n","    sa_quantiles = s_quantiles.gather(dim=2, index=action_index)\n","\n","    return sa_quantiles\n","\n","\n","\n","class DQNAgent(BaseAgent):\n","      def __init__(self, env, test_env, num_steps=5*(10**7),\n","                 batch_size=32, N=200, kappa=1.0, lr=5e-5, memory_size=10**6,\n","                 gamma=0.99, multi_step=1, update_interval=4,\n","                 target_update_interval=10000, start_steps=50000,\n","                 epsilon_train=0.01, epsilon_eval=0.001,\n","                 epsilon_decay_steps=250000, double_q_learning=False,\n","                 dueling_net=False, noisy_net=False, use_per=False,\n","                 log_interval=100, eval_interval=250000, num_eval_steps=125000,\n","                 max_episode_steps=27000, grad_cliping=None, cuda=True,\n","                 seed=0, num_quants=50):\n","        super(DQNAgent, self).__init__(\n","            env, test_env, num_steps, batch_size, memory_size,\n","            gamma, multi_step, update_interval, target_update_interval,\n","            start_steps, epsilon_train, epsilon_eval, epsilon_decay_steps,\n","            double_q_learning, dueling_net, noisy_net, use_per, log_interval,\n","            eval_interval, num_eval_steps, max_episode_steps, grad_cliping,\n","            cuda, seed)\n","\n","\n","        self.online_net = QRDQN(\n","              num_channels=env.observation_space.shape[0], num_actions=self.num_actions, num_quants=self.num_quants).to(self.device)\n","\n","\n","        self.target_net = QRDQN(\n","              num_channels=env.observation_space.shape[0], num_actions=self.num_actions, num_quants=self.num_quants).to(self.device)\n","\n","        # Copy parameters of the learning network to the target network.\n","        self.update_target()\n","        # Disable calculations of gradients of the target network.\n","        disable_gradients(self.target_net)\n","        self.optim = Adam(\n","            self.online_net.parameters(),\n","            lr=lr, eps=1e-2/batch_size)\n","        self.loss_fn = torch.nn.MSELoss()\n","        self.num_quants = num_quants\n","\n","        taus = torch.arange(\n","            0, self.num_quants+1, device=self.device, dtype=torch.float32) / self.num_quants\n","        self.tau_hats = ((taus[1:] + taus[:-1]) / 2.0).view(1, self.num_quants)\n","        self.kappa = kappa\n","\n","      def learn(self):\n","        self.learning_steps += 1\n","        states, actions, rewards, next_states, dones =\\\n","            self.memory.sample(self.batch_size)\n","        weights = None\n","\n","        quantile_loss, mean_q, errors = self.calculate_loss(\n","            states, actions, rewards, next_states, dones, weights)\n","        self.optim.zero_grad()\n","        quantile_loss.backward()\n","        # Clip norms of gradients to stebilize training.\n","        self.optim.step()\n","\n","\n","\n","\n","      def calculate_loss(self, states, actions, rewards, next_states, dones, weights):\n","\n","        current_sa_quantiles = evaluate_quantile_at_action(\n","            self.online_net(states=states), actions)\n","        assert current_sa_quantiles.shape == (self.batch_size, self.num_quants, 1)\n","\n","        with torch.no_grad():\n","            # Calculate Q values of next states.\n","            next_q = self.online_net.calculate_q(states=next_states)\n","\n","            # Calculate greedy actions.\n","            next_actions = torch.argmax(next_q, dim=1, keepdim=True)\n","            assert next_actions.shape == (self.batch_size, 1)\n","\n","            # Calculate quantile values of next states and actions at tau_hats.\n","            next_sa_quantiles = evaluate_quantile_at_action(\n","                self.target_net(states=next_states),\n","                next_actions).transpose(1, 2)\n","            assert next_sa_quantiles.shape == (self.batch_size, 1, self.num_quants)\n","\n","            # Calculate target quantile values.\n","            target_sa_quantiles = rewards[..., None] + (\n","                1.0 - dones[..., None]) * self.gamma * next_sa_quantiles\n","            assert target_sa_quantiles.shape == (self.batch_size, 1, self.num_quants)\n","\n","        td_errors = target_sa_quantiles - current_sa_quantiles\n","        assert td_errors.shape == (self.batch_size, self.num_quants, self.num_quants)\n","\n","        quantile_huber_loss = calculate_quantile_huber_loss(\n","            td_errors, self.tau_hats, weights, self.kappa)\n","\n","        return quantile_huber_loss, next_q.detach().mean().item(), \\\n","            td_errors.detach().abs().sum(dim=1).mean(dim=1, keepdim=True)"],"metadata":{"id":"FJvMsYC_0Z3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Training and Testing Environments\n","env = make_pytorch_env(\"BreakoutNoFrameskip-v4\")\n","test_env = make_pytorch_env(\"BreakoutNoFrameskip-v4\", episode_life=False, clip_rewards=False)\n"],"metadata":{"id":"drzebIXVqyUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start Training\n","agent = DQNAgent(env=env, test_env=test_env)\n","agent.run()"],"metadata":{"id":"Gmc2MD4TqzjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function used for saving weights\n","weight_path = \"qnet.pth\"\n","torch.save(agent.online_net.state_dict(), weight_path)"],"metadata":{"id":"5-GwFl4OU_Ys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"rl_coding_proj3\"\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","actor_path = \"weight_50.pth\"\n","path = os.path.join(GOOGLE_DRIVE_PATH, actor_path)"],"metadata":{"id":"a4RFeYGDtsiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = make_pytorch_env(\"BreakoutNoFrameskip-v4\")\n","test_env = make_pytorch_env(\"BreakoutNoFrameskip-v4\", episode_life=False, clip_rewards=False)\n","agent = Agent1(env=env, test_env=test_env)\n","\n","import time\n","start_time = time.time()\n","def evaluate():\n","    num_episodes = 0\n","    num_steps = 0\n","    total_return = 0.0\n","    while True:\n","        state = test_env.reset()\n","        episode_steps = 0\n","        episode_return = 0.0\n","        done = False\n","        print(\"number of episodes: \", num_episodes)\n","        while (not done) and episode_steps <= 27000:\n","            state = torch.ByteTensor(state).unsqueeze(0).to(agent.device).float() / 255.\n","            with torch.no_grad():\n","                action = agent.online_net.calculate_q(states=state).argmax().item()\n","            next_state, reward, done, _ = test_env.step(action)\n","            num_steps += 1\n","            episode_steps += 1\n","            episode_return += reward\n","            state = next_state\n","            print(\"number of steps: \", num_steps)\n","            print(\"number of episode steps: \", episode_steps)\n","            print(\"reward: \", episode_return)\n","        num_episodes += 1\n","        total_return += episode_return\n","\n","\n","\n","        if num_steps > 120000:\n","            break\n","\n","    mean_return = total_return / num_episodes\n","    print('-' * 60)\n","    print(f'return: {mean_return:<5.1f}')\n","    print('-' * 60)\n","\n","evaluate()\n","print(time.time()-start_time)"],"metadata":{"id":"eMGpfsNvVN-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gym\n","import random\n","import matplotlib.pyplot as plt\n","from colabgymrender.recorder import Recorder\n","\n","env = make_pytorch_env(\"BreakoutNoFrameskip-v4\")\n","directory = './video'\n","env = Recorder(env, directory)\n","state = env.reset()\n","terminal = False\n","while not terminal:\n","    state = torch.ByteTensor(state).unsqueeze(0).to(agent.device).float() / 255.\n","    with torch.no_grad():\n","        action = agent.online_net(states=state).argmax().item()\n","    state,r,terminal,info = env.step(action)\n","env.play()"],"metadata":{"id":"SH8tSchTdrDz"},"execution_count":null,"outputs":[]}]}